{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03026391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report: Non-Fine-Tuned Random Forest ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92    110077\n",
      "           1       0.93      0.94      0.93    120649\n",
      "\n",
      "    accuracy                           0.93    230726\n",
      "   macro avg       0.93      0.93      0.93    230726\n",
      "weighted avg       0.93      0.93      0.93    230726\n",
      "\n",
      "\n",
      "=== Classification Report: Fine-Tuned Random Forest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=7)]: Done 161 out of 161 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97    110077\n",
      "           1       0.98      0.97      0.98    120649\n",
      "\n",
      "    accuracy                           0.97    230726\n",
      "   macro avg       0.97      0.98      0.97    230726\n",
      "weighted avg       0.98      0.97      0.97    230726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import load\n",
    "\n",
    "# === Load embeddings and IDs ===\n",
    "\n",
    "# Cancerous\n",
    "emb_cancer = np.load(\"/home/azureuser/dna_sequencing/model_training/embeddings_forw_can.npy\")\n",
    "ids_cancer = pd.read_csv(\"/home/azureuser/dna_sequencing/model_training/embeddings_ids.csv\")[\"id\"]\n",
    "labels_cancer = np.ones(len(ids_cancer), dtype=int)  # label 1\n",
    "\n",
    "# Non-cancerous\n",
    "emb_noncan = np.load(\"/home/azureuser/dna_sequencing/model_training/embeddings_forw_noncan.npy\")\n",
    "ids_noncan = pd.read_csv(\"/home/azureuser/dna_sequencing/model_training/embeddings_ids2.csv\")[\"id\"]\n",
    "labels_noncan = np.zeros(len(ids_noncan), dtype=int)  # label 0\n",
    "\n",
    "# === Combine all ===\n",
    "X = np.vstack([emb_cancer, emb_noncan])\n",
    "all_ids = pd.concat([ids_cancer, ids_noncan], ignore_index=True)\n",
    "all_labels = np.concatenate([labels_cancer, labels_noncan])\n",
    "\n",
    "# === Create and Shuffle DataFrame ===\n",
    "df_combined = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": all_labels,\n",
    "    \"embedding\": list(X)\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n",
    "\n",
    "# === Convert for Model Input ===\n",
    "X_all = np.vstack(df_combined[\"embedding\"].values)\n",
    "y_all = df_combined[\"label\"].values\n",
    "\n",
    "# === Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, stratify=y_all, random_state=42\n",
    ")\n",
    "\n",
    "# === Load Models ===\n",
    "model_non_fine = load(\"/home/azureuser/dna_sequencing/model_training/random_forest_dnabert_model.joblib\")\n",
    "model_fine = load(\"/home/azureuser/dna_sequencing/model_training/forw_final_randomforest_optuna_model.joblib\")\n",
    "\n",
    "# === Predict and Evaluate ===\n",
    "print(\"=== Classification Report: Non-Fine-Tuned Random Forest ===\")\n",
    "y_pred_non_fine = model_non_fine.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_non_fine))\n",
    "\n",
    "print(\"\\n=== Classification Report: Fine-Tuned Random Forest ===\")\n",
    "y_pred_fine = model_fine.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_fine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc974f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Fine-Tuned Accuracy: 0.9273\n",
      "Fine-Tuned Accuracy: 0.9750\n",
      "Non-Fine-Tuned AUC: 0.9268\n",
      "Fine-Tuned AUC: 0.9753\n",
      "\n",
      "Confusion Matrix - Non-Fine-Tuned:\n",
      "[[101049   9028]\n",
      " [  7756 112893]]\n",
      "\n",
      "Confusion Matrix - Fine-Tuned:\n",
      "[[108047   2030]\n",
      " [  3747 116902]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "\n",
    "# Add these after your classification reports\n",
    "print(f\"Non-Fine-Tuned Accuracy: {accuracy_score(y_test, y_pred_non_fine):.4f}\")\n",
    "print(f\"Fine-Tuned Accuracy: {accuracy_score(y_test, y_pred_fine):.4f}\")\n",
    "\n",
    "print(f\"Non-Fine-Tuned AUC: {roc_auc_score(y_test, y_pred_non_fine):.4f}\")\n",
    "print(f\"Fine-Tuned AUC: {roc_auc_score(y_test, y_pred_fine):.4f}\")\n",
    "\n",
    "# Confusion matrices\n",
    "print(\"\\nConfusion Matrix - Non-Fine-Tuned:\")\n",
    "print(confusion_matrix(y_test, y_pred_non_fine))\n",
    "print(\"\\nConfusion Matrix - Fine-Tuned:\")\n",
    "print(confusion_matrix(y_test, y_pred_fine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4c8c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report: Random Forest (Backward) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    109641\n",
      "           1       0.99      1.00      1.00    120612\n",
      "\n",
      "    accuracy                           1.00    230253\n",
      "   macro avg       1.00      1.00      1.00    230253\n",
      "weighted avg       1.00      1.00      1.00    230253\n",
      "\n",
      "\n",
      "=== Classification Report: Neural Network (Backward) ===\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87    109641\n",
      "           1       0.87      0.91      0.89    120612\n",
      "\n",
      "    accuracy                           0.88    230253\n",
      "   macro avg       0.88      0.88      0.88    230253\n",
      "weighted avg       0.88      0.88      0.88    230253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import load\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === Load embeddings and IDs ===\n",
    "\n",
    "# Cancerous\n",
    "emb_cancer = np.load(\"/home/azureuser/dna_sequencing/model_training/embeddings_backw_can.npy\")\n",
    "ids_cancer = pd.read_csv(\"/home/azureuser/dna_sequencing/model_training/backw_can_embeddings_ids.csv\")[\"id\"]\n",
    "labels_cancer = np.ones(len(ids_cancer), dtype=int)\n",
    "\n",
    "# Non-cancerous\n",
    "emb_noncan = np.load(\"/home/azureuser/dna_sequencing/model_training/embeddings_backw_noncan.npy\")\n",
    "ids_noncan = pd.read_csv(\"/home/azureuser/dna_sequencing/model_training/backw_noncan_embeddings_ids.csv\")[\"id\"]\n",
    "labels_noncan = np.zeros(len(ids_noncan), dtype=int)\n",
    "\n",
    "# === Combine all ===\n",
    "X = np.vstack([emb_cancer, emb_noncan])\n",
    "all_ids = pd.concat([ids_cancer, ids_noncan], ignore_index=True)\n",
    "all_labels = np.concatenate([labels_cancer, labels_noncan])\n",
    "\n",
    "# === Create and Shuffle DataFrame ===\n",
    "df_combined = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": all_labels,\n",
    "    \"embedding\": list(X)\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === Convert for Model Input ===\n",
    "X_all = np.vstack(df_combined[\"embedding\"].values)\n",
    "y_all = df_combined[\"label\"].values\n",
    "\n",
    "# === Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, stratify=y_all, random_state=42\n",
    ")\n",
    "\n",
    "# === Load Models ===\n",
    "rf_model = load(\"/home/azureuser/dna_sequencing/model_training/backw_final_randomforest_optuna_model.joblib\")\n",
    "nn_model = load_model(\"/home/azureuser/dna_sequencing/model_training/backww_final_nn_model.keras\")\n",
    "\n",
    "# === Predict and Evaluate: Random Forest ===\n",
    "print(\"=== Classification Report: Random Forest (Backward) ===\")\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# === Predict and Evaluate: Neural Network ===\n",
    "print(\"\\n=== Classification Report: Neural Network (Backward) ===\")\n",
    "y_pred_nn = nn_model.predict(X_test, batch_size=256)\n",
    "y_pred_nn_labels = (y_pred_nn > 0.5).astype(int).flatten()  # assuming binary classification\n",
    "print(classification_report(y_test, y_pred_nn_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff524c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC (probabilities): 1.0000\n",
      "\n",
      "Confusion Matrix - Random Forest:\n",
      "[[108861    780]\n",
      " [    32 120580]]\n",
      "Neural Network AUC (probabilities): 0.9493\n",
      "\n",
      "Confusion Matrix - Neural Network:\n",
      "[[ 92815  16826]\n",
      " [ 10669 109943]]\n",
      "\n",
      "==================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "==================================================\n",
      "Random Forest - Accuracy: 0.9965, AUC: 1.0000\n",
      "Best AUC: Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Random Forest AUC (using probabilities if available)\n",
    "if hasattr(rf_model, 'predict_proba'):\n",
    "    y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"Random Forest AUC (probabilities): {roc_auc_score(y_test, y_proba_rf):.4f}\")\n",
    "else:\n",
    "    print(f\"Random Forest AUC (predictions): {roc_auc_score(y_test, y_pred_rf):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix - Random Forest:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# Neural Network AUC (using probabilities from predict)\n",
    "y_proba_nn = y_pred_nn.flatten()  # NN predict gives probabilities\n",
    "print(f\"Neural Network AUC (probabilities): {roc_auc_score(y_test, y_proba_nn):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix - Neural Network:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn_labels))\n",
    "\n",
    "# === Model Comparison Summary ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "nn_acc = accuracy_score(y_test, y_pred_nn_labels)\n",
    "\n",
    "if hasattr(rf_model, 'predict_proba'):\n",
    "    rf_auc = roc_auc_score(y_test, y_proba_rf)\n",
    "else:\n",
    "    rf_auc = roc_auc_score(y_test, y_pred_rf)\n",
    "\n",
    "nn_auc = roc_auc_score(y_test, y_proba_nn)\n",
    "\n",
    "print(f\"Random Forest - Accuracy: {rf_acc:.4f}, AUC: {rf_auc:.4f}\")\n",
    "print(f\"Best AUC: {'Random Forest' if rf_auc > nn_auc else 'Neural Network'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bc554",
   "metadata": {},
   "source": [
    "### Checking Data Leakage for Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42200122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA LEAKAGE DETECTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. CHECKING FOR ID OVERLAP BETWEEN CANCER AND NON-CANCER DATASETS\n",
      "-------------------------------------------------------\n",
      "Cancer IDs: 603058\n",
      "Non-cancer IDs: 548205\n",
      "Overlapping IDs: 0\n",
      "✅ No ID overlap between cancer and non-cancer datasets\n",
      "\n",
      "2. CHECKING FOR DUPLICATE IDs WITHIN DATASETS\n",
      "---------------------------------------------\n",
      "Duplicate IDs in cancer dataset: 0\n",
      "Duplicate IDs in non-cancer dataset: 0\n",
      "✅ No duplicate IDs within individual datasets\n",
      "\n",
      "3. CHECKING FOR TRAIN-TEST ID OVERLAP\n",
      "-----------------------------------\n",
      "Training IDs: 921010\n",
      "Test IDs: 230253\n",
      "Overlapping IDs: 0\n",
      "✅ No ID overlap between train and test sets\n",
      "\n",
      "4. CHECKING FOR PATIENT-LEVEL LEAKAGE\n",
      "-----------------------------------\n",
      "Sample ID -> Patient ID extraction:\n",
      "  SRR5177930.6 -> SRR5177930\n",
      "  SRR5177930.9 -> SRR5177930\n",
      "  SRR5177930.11 -> SRR5177930\n",
      "  SRR5177930.16 -> SRR5177930\n",
      "  SRR5177930.20 -> SRR5177930\n",
      "\n",
      "Unique patients in training: 2\n",
      "Unique patients in test: 2\n",
      "Overlapping patients: 2\n",
      "🚨 WARNING: Same patients in both train and test sets!\n",
      "First 10 overlapping patients: ['SRR5177930', 'SRR6269879']\n",
      "This suggests patient-level data leakage!\n",
      "\n",
      "5. CHECKING EMBEDDING SIMILARITY\n",
      "------------------------------\n",
      "Comparing 1000 test samples with 1000 train samples...\n",
      "Test samples with >95% similarity to training: 24/1000 (2.4%)\n",
      "Test samples with >99% similarity to training: 0/1000 (0.0%)\n",
      "Max similarity found: 0.974519\n",
      "Mean max similarity: 0.742475\n",
      "Std of max similarities: 0.114253\n",
      "✅ Embedding similarities look reasonable\n",
      "\n",
      "6. PERFORMANCE REALITY CHECK\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9965\n",
      "Neural Network Accuracy: 0.8806\n",
      "🚨 CRITICAL: Near-perfect accuracy detected!\n",
      "This is highly suspicious and likely indicates data leakage!\n",
      "\n",
      "6b. CROSS-VALIDATION CONSISTENCY CHECK\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 30.3min\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed: 32.4min finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 30.5min\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed: 32.6min finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 30.4min\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed: 32.5min finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 31.0min\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed: 33.1min finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 30.5min\n",
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed: 32.7min finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    4.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "  Single split accuracy: 0.9965\n",
      "  CV mean accuracy: 0.8351 (+/- 0.0018)\n",
      "  🚨 WARNING: Single split much better than CV - possible leakage!\n",
      "\n",
      "============================================================\n",
      "SUMMARY AND RECOMMENDATIONS\n",
      "============================================================\n",
      "POTENTIAL DATA LEAKAGE DETECTED:\n",
      "  🚨 WARNING: Same patients in train and test sets\n",
      "  🚨 CRITICAL: Near-perfect model performance\n",
      "\n",
      "RECOMMENDED ACTIONS:\n",
      "1. Remove any duplicate or overlapping IDs\n",
      "2. Ensure train-test split at patient level, not sample level\n",
      "3. Verify that embeddings were generated independently\n",
      "4. Re-evaluate models after fixing leakage issues\n",
      "5. Consider using temporal splits if time information available\n",
      "\n",
      "Note: This analysis covers common leakage patterns.\n",
      "Domain-specific leakage might require additional checks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done 200 out of 200 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "# === Data Leakage Detection Script ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import load\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Load your data (same as your original code) ===\n",
    "# Cancerous\n",
    "emb_cancer = np.load(\"/home/azureuser/dna_sequencing/model_training/embeddings_backw_can.npy\")\n",
    "ids_cancer = pd.read_csv(\"/home/azureuser/dna_sequencing/model_training/backw_can_embeddings_ids.csv\")[\"id\"]\n",
    "labels_cancer = np.ones(len(ids_cancer), dtype=int)\n",
    "\n",
    "# Non-cancerous\n",
    "emb_noncan = np.load(\"/home/azureuser/dna_sequencing/model_training/embeddings_backw_noncan.npy\")\n",
    "ids_noncan = pd.read_csv(\"/home/azureuser/dna_sequencing/model_training/backw_noncan_embeddings_ids.csv\")[\"id\"]\n",
    "labels_noncan = np.zeros(len(ids_noncan), dtype=int)\n",
    "\n",
    "# Combine all\n",
    "X = np.vstack([emb_cancer, emb_noncan])\n",
    "all_ids = pd.concat([ids_cancer, ids_noncan], ignore_index=True)\n",
    "all_labels = np.concatenate([labels_cancer, labels_noncan])\n",
    "\n",
    "# Create DataFrame\n",
    "df_combined = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"label\": all_labels,\n",
    "    \"embedding\": list(X)\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X_all = np.vstack(df_combined[\"embedding\"].values)\n",
    "y_all = df_combined[\"label\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test, train_ids, test_ids = train_test_split(\n",
    "    X_all, y_all, df_combined[\"id\"], test_size=0.2, stratify=y_all, random_state=42\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LEAKAGE DETECTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === 1. Check for Identical IDs Between Cancer and Non-Cancer ===\n",
    "print(\"\\n1. CHECKING FOR ID OVERLAP BETWEEN CANCER AND NON-CANCER DATASETS\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "cancer_ids_set = set(ids_cancer)\n",
    "noncan_ids_set = set(ids_noncan)\n",
    "id_overlap = cancer_ids_set.intersection(noncan_ids_set)\n",
    "\n",
    "print(f\"Cancer IDs: {len(cancer_ids_set)}\")\n",
    "print(f\"Non-cancer IDs: {len(noncan_ids_set)}\")\n",
    "print(f\"Overlapping IDs: {len(id_overlap)}\")\n",
    "\n",
    "if id_overlap:\n",
    "    print(\"🚨 WARNING: Same IDs found in both cancer and non-cancer datasets!\")\n",
    "    print(f\"First 10 overlapping IDs: {list(id_overlap)[:10]}\")\n",
    "    print(\"This is DEFINITE data leakage - same samples labeled as both cancer and non-cancer!\")\n",
    "else:\n",
    "    print(\"✅ No ID overlap between cancer and non-cancer datasets\")\n",
    "\n",
    "# === 2. Check for Duplicate IDs Within Each Dataset ===\n",
    "print(\"\\n2. CHECKING FOR DUPLICATE IDs WITHIN DATASETS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "cancer_duplicates = ids_cancer[ids_cancer.duplicated()]\n",
    "noncan_duplicates = ids_noncan[ids_noncan.duplicated()]\n",
    "\n",
    "print(f\"Duplicate IDs in cancer dataset: {len(cancer_duplicates)}\")\n",
    "print(f\"Duplicate IDs in non-cancer dataset: {len(noncan_duplicates)}\")\n",
    "\n",
    "if len(cancer_duplicates) > 0:\n",
    "    print(\"🚨 WARNING: Duplicate IDs found in cancer dataset!\")\n",
    "    print(f\"First 10 duplicates: {list(cancer_duplicates[:10])}\")\n",
    "\n",
    "if len(noncan_duplicates) > 0:\n",
    "    print(\"🚨 WARNING: Duplicate IDs found in non-cancer dataset!\")\n",
    "    print(f\"First 10 duplicates: {list(noncan_duplicates[:10])}\")\n",
    "\n",
    "if len(cancer_duplicates) == 0 and len(noncan_duplicates) == 0:\n",
    "    print(\"✅ No duplicate IDs within individual datasets\")\n",
    "\n",
    "# === 3. Check Train-Test ID Overlap ===\n",
    "print(\"\\n3. CHECKING FOR TRAIN-TEST ID OVERLAP\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "train_ids_set = set(train_ids)\n",
    "test_ids_set = set(test_ids)\n",
    "train_test_overlap = train_ids_set.intersection(test_ids_set)\n",
    "\n",
    "print(f\"Training IDs: {len(train_ids_set)}\")\n",
    "print(f\"Test IDs: {len(test_ids_set)}\")\n",
    "print(f\"Overlapping IDs: {len(train_test_overlap)}\")\n",
    "\n",
    "if train_test_overlap:\n",
    "    print(\"🚨 CRITICAL: Same IDs in both train and test sets!\")\n",
    "    print(f\"Overlapping IDs: {list(train_test_overlap)[:10]}\")\n",
    "else:\n",
    "    print(\"✅ No ID overlap between train and test sets\")\n",
    "\n",
    "# === 4. Check Patient-Level Leakage ===\n",
    "print(\"\\n4. CHECKING FOR PATIENT-LEVEL LEAKAGE\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "def extract_patient_id(seq_id):\n",
    "    \"\"\"Extract patient ID from sequence ID - adjust based on your ID format\"\"\"\n",
    "    # Common patterns in biological data:\n",
    "    # TCGA-XX-XXXX-XXX-XXX-XXXX-XX\n",
    "    # Patient_001_Sample_A\n",
    "    # SRR123456.1\n",
    "    \n",
    "    seq_id_str = str(seq_id)\n",
    "    \n",
    "    # TCGA format\n",
    "    if seq_id_str.startswith('TCGA'):\n",
    "        parts = seq_id_str.split('-')\n",
    "        if len(parts) >= 3:\n",
    "            return f\"{parts[0]}-{parts[1]}-{parts[2]}\"  # TCGA-XX-XXXX\n",
    "    \n",
    "    # Patient_Sample format\n",
    "    if 'Patient' in seq_id_str or 'patient' in seq_id_str:\n",
    "        parts = seq_id_str.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[0]}_{parts[1]}\"\n",
    "    \n",
    "    # SRR format (sequence read archive)\n",
    "    if seq_id_str.startswith('SRR') or seq_id_str.startswith('ERR'):\n",
    "        return seq_id_str.split('.')[0]\n",
    "    \n",
    "    # Generic underscore/dash separation\n",
    "    if '_' in seq_id_str:\n",
    "        return seq_id_str.split('_')[0]\n",
    "    elif '-' in seq_id_str:\n",
    "        return seq_id_str.split('-')[0]\n",
    "    \n",
    "    # If no pattern matches, return first part or whole ID\n",
    "    return seq_id_str.split()[0] if ' ' in seq_id_str else seq_id_str\n",
    "\n",
    "# Show some examples of patient ID extraction\n",
    "print(\"Sample ID -> Patient ID extraction:\")\n",
    "sample_ids = list(all_ids)[:5]\n",
    "for sid in sample_ids:\n",
    "    pid = extract_patient_id(sid)\n",
    "    print(f\"  {sid} -> {pid}\")\n",
    "\n",
    "# Extract patient IDs\n",
    "train_patients = set([extract_patient_id(id_) for id_ in train_ids])\n",
    "test_patients = set([extract_patient_id(id_) for id_ in test_ids])\n",
    "patient_overlap = train_patients.intersection(test_patients)\n",
    "\n",
    "print(f\"\\nUnique patients in training: {len(train_patients)}\")\n",
    "print(f\"Unique patients in test: {len(test_patients)}\")\n",
    "print(f\"Overlapping patients: {len(patient_overlap)}\")\n",
    "\n",
    "if patient_overlap:\n",
    "    print(\"🚨 WARNING: Same patients in both train and test sets!\")\n",
    "    print(f\"First 10 overlapping patients: {list(patient_overlap)[:10]}\")\n",
    "    print(\"This suggests patient-level data leakage!\")\n",
    "else:\n",
    "    print(\"✅ No patient overlap between train and test sets\")\n",
    "\n",
    "# === 5. Embedding Similarity Check ===\n",
    "print(\"\\n5. CHECKING EMBEDDING SIMILARITY\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Sample a subset for similarity check (full comparison might be too expensive)\n",
    "sample_size = min(1000, len(X_test), len(X_train))\n",
    "test_sample_idx = np.random.choice(len(X_test), min(sample_size, len(X_test)), replace=False)\n",
    "train_sample_idx = np.random.choice(len(X_train), min(sample_size, len(X_train)), replace=False)\n",
    "\n",
    "X_test_sample = X_test[test_sample_idx]\n",
    "X_train_sample = X_train[train_sample_idx]\n",
    "\n",
    "print(f\"Comparing {len(X_test_sample)} test samples with {len(X_train_sample)} train samples...\")\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarities = cosine_similarity(X_test_sample, X_train_sample)\n",
    "max_similarities = np.max(similarities, axis=1)\n",
    "\n",
    "# Check for suspiciously high similarities\n",
    "threshold_95 = 0.95\n",
    "threshold_99 = 0.99\n",
    "\n",
    "high_sim_95 = np.sum(max_similarities > threshold_95)\n",
    "high_sim_99 = np.sum(max_similarities > threshold_99)\n",
    "\n",
    "print(f\"Test samples with >95% similarity to training: {high_sim_95}/{len(X_test_sample)} ({high_sim_95/len(X_test_sample)*100:.1f}%)\")\n",
    "print(f\"Test samples with >99% similarity to training: {high_sim_99}/{len(X_test_sample)} ({high_sim_99/len(X_test_sample)*100:.1f}%)\")\n",
    "print(f\"Max similarity found: {np.max(max_similarities):.6f}\")\n",
    "print(f\"Mean max similarity: {np.mean(max_similarities):.6f}\")\n",
    "print(f\"Std of max similarities: {np.std(max_similarities):.6f}\")\n",
    "\n",
    "if high_sim_99 > 0:\n",
    "    print(\"🚨 CRITICAL: Very high embedding similarities detected!\")\n",
    "    print(\"This strongly suggests identical or near-identical sequences in train/test!\")\n",
    "elif high_sim_95 > len(X_test_sample) * 0.1:  # More than 10%\n",
    "    print(\"🚨 WARNING: Many high embedding similarities detected!\")\n",
    "    print(\"This may indicate data leakage or very similar sequences!\")\n",
    "else:\n",
    "    print(\"✅ Embedding similarities look reasonable\")\n",
    "\n",
    "# === 6. Performance Reality Check ===\n",
    "print(\"\\n6. PERFORMANCE REALITY CHECK\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "# Load models and make predictions\n",
    "try:\n",
    "    rf_model = load(\"/home/azureuser/dna_sequencing/model_training/backw_final_randomforest_optuna_model.joblib\")\n",
    "    nn_model = load_model(\"/home/azureuser/dna_sequencing/model_training/backww_final_nn_model.keras\")\n",
    "    \n",
    "    # Random Forest predictions\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "    \n",
    "    # Neural Network predictions\n",
    "    y_pred_nn = nn_model.predict(X_test, batch_size=256, verbose=0)\n",
    "    y_pred_nn_labels = (y_pred_nn > 0.5).astype(int).flatten()\n",
    "    nn_acc = accuracy_score(y_test, y_pred_nn_labels)\n",
    "    \n",
    "    print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
    "    print(f\"Neural Network Accuracy: {nn_acc:.4f}\")\n",
    "    \n",
    "    # Flag suspiciously high performance\n",
    "    if rf_acc > 0.99 or nn_acc > 0.99:\n",
    "        print(\"🚨 CRITICAL: Near-perfect accuracy detected!\")\n",
    "        print(\"This is highly suspicious and likely indicates data leakage!\")\n",
    "    elif rf_acc > 0.95 or nn_acc > 0.95:\n",
    "        print(\"🚨 WARNING: Very high accuracy detected!\")\n",
    "        print(\"Verify this is realistic for your specific domain and dataset\")\n",
    "    else:\n",
    "        print(\"✅ Performance levels seem reasonable\")\n",
    "        \n",
    "    # Cross-validation consistency check\n",
    "    print(\"\\n6b. CROSS-VALIDATION CONSISTENCY CHECK\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    try:\n",
    "        cv_scores_rf = cross_val_score(rf_model, X_all, y_all, cv=5, scoring='accuracy')\n",
    "        cv_mean_rf = cv_scores_rf.mean()\n",
    "        cv_std_rf = cv_scores_rf.std()\n",
    "        \n",
    "        print(f\"Random Forest:\")\n",
    "        print(f\"  Single split accuracy: {rf_acc:.4f}\")\n",
    "        print(f\"  CV mean accuracy: {cv_mean_rf:.4f} (+/- {cv_std_rf * 2:.4f})\")\n",
    "        \n",
    "        if rf_acc > cv_mean_rf + 2*cv_std_rf:\n",
    "            print(\"  🚨 WARNING: Single split much better than CV - possible leakage!\")\n",
    "        else:\n",
    "            print(\"  ✅ Single split consistent with CV\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform CV check: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load models for performance check: {e}\")\n",
    "\n",
    "# === 7. Summary and Recommendations ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "leakage_flags = []\n",
    "\n",
    "if id_overlap:\n",
    "    leakage_flags.append(\"🚨 CRITICAL: Same IDs in cancer and non-cancer datasets\")\n",
    "\n",
    "if len(cancer_duplicates) > 0 or len(noncan_duplicates) > 0:\n",
    "    leakage_flags.append(\"🚨 WARNING: Duplicate IDs within datasets\")\n",
    "\n",
    "if train_test_overlap:\n",
    "    leakage_flags.append(\"🚨 CRITICAL: Same IDs in train and test sets\")\n",
    "\n",
    "if patient_overlap:\n",
    "    leakage_flags.append(\"🚨 WARNING: Same patients in train and test sets\")\n",
    "\n",
    "if high_sim_99 > 0:\n",
    "    leakage_flags.append(\"🚨 CRITICAL: Very high embedding similarities\")\n",
    "\n",
    "try:\n",
    "    if 'rf_acc' in locals() and 'nn_acc' in locals():\n",
    "        if rf_acc > 0.99 or nn_acc > 0.99:\n",
    "            leakage_flags.append(\"🚨 CRITICAL: Near-perfect model performance\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if leakage_flags:\n",
    "    print(\"POTENTIAL DATA LEAKAGE DETECTED:\")\n",
    "    for flag in leakage_flags:\n",
    "        print(f\"  {flag}\")\n",
    "    \n",
    "    print(\"\\nRECOMMENDED ACTIONS:\")\n",
    "    print(\"1. Remove any duplicate or overlapping IDs\")\n",
    "    print(\"2. Ensure train-test split at patient level, not sample level\")\n",
    "    print(\"3. Verify that embeddings were generated independently\")\n",
    "    print(\"4. Re-evaluate models after fixing leakage issues\")\n",
    "    print(\"5. Consider using temporal splits if time information available\")\n",
    "    \n",
    "else:\n",
    "    print(\"✅ No obvious data leakage detected!\")\n",
    "    print(\"Your experimental setup appears to be sound.\")\n",
    "\n",
    "print(\"\\nNote: This analysis covers common leakage patterns.\")\n",
    "print(\"Domain-specific leakage might require additional checks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna_sequence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
